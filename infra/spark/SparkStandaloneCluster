FROM openjdk:11-jre-slim

ENV SPARK_VERSION=3.5.1
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH="$SPARK_HOME/bin:$PATH"

RUN apt-get update && apt-get install -y curl tar bash procps python3 python3-pip python3-venv git && apt-get clean

# Download and extract Apache Spark
RUN curl -o /tmp/spark.tgz "https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \
    && mkdir -p /opt \
    && tar -xzf /tmp/spark.tgz -C /opt \
    && mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} $SPARK_HOME \
    && rm /tmp/spark.tgz

# Debug: List the files to confirm
RUN ls -la $SPARK_HOME/bin

# Expose necessary ports
EXPOSE 4040 7077 8080 8081 6066

WORKDIR $SPARK_HOME

# Copy all necessary files into the container
COPY /build/jobs/* /home/ubuntu/kafka/jobs/
COPY /build/utils/* /home/ubuntu/kafka/utils/
COPY /build/requirements.txt /home/ubuntu/kafka/requirements.txt

# Set the working directory
WORKDIR /home/ubuntu/kafka

# # Create a virtual environment
# RUN python3 -m venv env

# # Install required Python packages
# RUN /bin/bash -c "source env/bin/activate"

RUN pip install -r "requirements.txt"

CMD ["spark-shell"]
